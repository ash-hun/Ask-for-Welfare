# Import Module
import streamlit as st
import chromadb
import torch
import re
import os
import io
import utility

from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

from streamlit_chat import message # Chatbot UI
from audio_recorder_streamlit import audio_recorder # ìŒì„±ë…¹ìŒ
from pydub import AudioSegment # ë…¹ìŒ íŒŒì¼ ì €ì¥
from openai import OpenAI # STT

# ====================================================================================================================
# Global Config
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY") # í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
score_threshold = 0.2
search_k = 5
llm_model = "gpt-4-1106-preview" # gpt-3.5-turbo / gpt-4-1106-preview
user_img = "https://freesvg.org/img/abstract-user-flat-4.png"
bot_img = "https://github.com/ash-hun/WelSSISKo/raw/main/assets/logo02.png"

st.set_page_config(
    page_title="ë¬¼ì–´ë³´ì¥",
    page_icon="ğŸ‘‹",
)

# GPU or CPU Device Setting
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

st.title("ë¬¼ì–´ë³´ì¥")

# ====================================================================================================================
# íŒŒì¸íŠœë‹í•œ ì„ë² ë”© ëª¨ë¸
model_dir = './embeddingModel' # í•„ìš”ì‹œ ê²½ë¡œë³€ê²½
embedding = SentenceTransformerEmbeddings(model_name=model_dir, model_kwargs={'device': device}, encode_kwargs={'normalize_embeddings':True})

# ChromaDB ë¶ˆëŸ¬ì˜¤ê¸°
chroma_client = chromadb.PersistentClient(path="chroma")

collection_name = "vector_db"
collection = chroma_client.get_collection(collection_name)

vectorstore = Chroma(
    client= chroma_client,
    collection_name= collection_name,
    embedding_function= embedding,
    persist_directory="./chroma"
)

# ì„ê³„ì  ê¸°ë°˜ : ì ì ˆí•œ threshold ê°’ ì„ ì •ì´ í•„ìˆ˜ì„.
retriever = vectorstore.as_retriever(search_type="similarity_score_threshold", search_kwargs={'k': search_k ,'score_threshold': score_threshold})

## llm ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model_name=llm_model, temperature=0)  # Modify model_name if you have access to GPT-4 / gpt-3.5-turbo / gpt-4-1106-preview

## llm í”„ë¡¬í”„íŒ…
# ê²€ìƒ‰ëœ ë¬¸ì¥ ë‚´ì—ì„œë§Œ ëŒ€ë‹µí•˜ë„ë¡ í•˜ê³  ë‚´ìš©ì„ ë°”ê¾¸ì§€ ëª»í•˜ê²Œ í”„ë¡¬í”„íŠ¸ ì‘ì„±

system_template="""Use the following pieces of context to answer the users question shortly.
Given the following summaries of a long document and a question, create a final answer with references ("source_documents"), use "source_documents" in capital letters regardless of the number of sources.
But Don't say word of source_documents.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
----------------
{context}

You MUST answer in Korean"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)

chain_type_kwargs = {"prompt": prompt}

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)

# ====================================================================================================================
# Define Function
def stt():
    ## (ë…¹ìŒ) ë§ˆì´í¬ ë²„íŠ¼ ë‘ë²ˆ ëˆ„ë¥´ë©´ ì‚¬ìš©ì ìŒì„±ì‹ í˜¸ mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥ - ./output.mp3
    audio_bytes = audio_recorder(text="")

    if audio_bytes:
        st.audio(audio_bytes, format="audio/wav")

    ## ë…¹ìŒ ì™„ë£Œë˜ë©´ mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  SST í•¨ìˆ˜ ì´ìš©í•˜ì—¬ text ë³€í™˜
    if audio_bytes is not None:
        ## mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥
        audio_segmant = AudioSegment.from_file(io.BytesIO(audio_bytes))
        # Export the audio file
        audio_segmant.export('./data/audio/output.mp3', format='mp3')

        # mp3 íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ STT ì ìš©
        client = OpenAI()
        sst_text = utility.STT("./data/audio/output.mp3", client)

        clean_text = sst_text.replace("\n", "")

        # ìœ ì € inputì°½ì— í…ìŠ¤íŠ¸ ì‹¬ì–´ì¤Œ.
        js = f"""
        <script>
        function insertText(dummy_var_to_force_repeat_execution) {{
        var chatInput = parent.document.querySelector('textarea[data-testid="stChatInput"]');
        var nativeInputValueSetter = Object.getOwnPropertyDescriptor(window.HTMLTextAreaElement.prototype, "value").set;
        nativeInputValueSetter.call(chatInput, "{clean_text}");
        var event = new Event('input', {{ bubbles: true}});
        chatInput.dispatchEvent(event);
        }}
        insertText({len(st.session_state['generated'])});
        </script>
        """
        st.components.v1.html(js)

        audio_bytes = None 

def tts(): # TTS ê¸°ëŠ¥
    try:
        # st.sidebar.write(final_response)
        utility.chat_output_value(final_response)
        audio_file = open('./output.mp3', 'rb')
        audio_bytes = audio_file.read()

        st.sidebar.audio(audio_bytes, format='audio/mp3')
    except:
        st.sidebar.write('ìµœê·¼ ë‹µë³€ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.')
        audio_file = open('./data/audio/output_error.mp3', 'rb')
        audio_bytes = audio_file.read()

        st.sidebar.audio(audio_bytes, format='audio/mp3')

def llm_chatbot(question):
    """ llm_chatbot

    ì‚¬ìš©ìê°€ ì¿¼ë¦¬(question)ë¥¼ ì…ë ¥í•˜ë©´ LangChainì„ í†µí•´ embedding ëª¨ë¸ì„ ê±°ì³ Vector DBì— ë“¤ì–´ê°„ ë¬¸ì„œë¥¼ Retrieverí•˜ì—¬
    ê´€ë ¨ì„±ì´ ê¹Šì€ ë¬¸ì„œë¥¼ ì°¾ëŠ”ë‹¤. ì´ë•Œ, ì°¾ì•„ë‚¸ ê²°ê³¼(ë¬¸ì„œ ê°œìˆ˜)ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ì²˜ë¦¬ë¥¼ ì´í–‰í•œë‹¤.

    Args:
        question (str): _description_

    Returns:
        _type_: _description_
    """
    query = question
    result = chain(query)

    # ë¬¸ì„œ ê²€ìƒ‰ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
    if len(result['source_documents']) > 0: # ë¬¸ì„œ í•˜ë‚˜ë¼ë„ ê²€ìƒ‰ëœ ê²½ìš°
        # title ë°˜í™˜ì„ ìœ„í•œ ì½”ë“œ 
        lst = []
        for i in range(len(result['source_documents'])):
            try:
                # ì‹œë„: metadata['title']ì— ì ‘ê·¼
                title_link = "[" + result['source_documents'][i].metadata['title'] + "](https://www.bokjiro.go.kr/ssis-tbu/index.do)"
                lst.append(title_link)
            except KeyError:
                # ì˜ˆì™¸ ì²˜ë¦¬: 'title' í‚¤ê°€ ì—†ì„ ê²½ìš°
                continue
        return(result['result'], lst)
    else: # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ëŠ”ê²½ìš°
        return ((f"'{result['query']}' ì— ëŒ€í•œ ë‚´ìš©ì€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤."), '')

def set_list(docs):
    """ set_list
    ë¬¸ì„œ ë‚´ìš©ì´ ì¤‘ë³µë  ê²½ìš° ì œê±°í•œë‹¤.

    Args:
        docs (_type_) : None check duplicate data

    Returns:
        unique_list (list) : Delete duplicate data
    """

    unique_list = []
    seen = set()

    for item in docs:
        if item not in seen:
            unique_list.append(item)
            seen.add(item)
    return unique_list

def modeloutput(prompt):
    """ modeloutput

    ì‹¤ì œ ì¶œë ¥ë  ì±—ë´‡ë‚´ìš©ì„ ì •ì œí•œë‹¤.

    Args:
        prompt (str): Output prompt

    Returns:
        str : Transform output prompt
    """
    prompt, docs = llm_chatbot(prompt)
    prompt = re.sub(r'\[source_documents\]|\(source_documents\)|source_documents', '', prompt)
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì—°ê²°
    if len(docs) == 0:
        return (f"{prompt}", f"ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ê²€ìƒ‰ í•´ë³´ì‹œê±°ë‚˜, 'aaa@aaa.com'ì„ í†µí•´ ë¬¸ì˜ ë°”ëë‹ˆë‹¤.")
    else: 
        joined_docs = ', '.join(map(str, set_list(docs)))
        return (f"{prompt}", f"ì´ì™€ ê´€ë ¨ëœ ë³µì§€ì œë„ëŠ” **{joined_docs}** ë“±ì´ ìˆìŠµë‹ˆë‹¤.")

# Main Contents
# $ streamlit run prototype.py
if __name__ == "__main__":
    # =================================================================
    ## Setting Styling
    with open('./css.css', 'r', encoding='utf-8') as file:
        css = file.read()
    st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)
    # =================================================================
    # Layout Grid
    col1, col2= st.sidebar.columns(2)
    
    utility.add_logo()
    st.markdown(f"Version 0.2 / LLM : {llm_model}")
    # =================================================================
    with st.sidebar.container():
        with col1:
            # st.sidebar.button("ğŸ¤", on_click=stt) #ğŸ¤
            ## (ë…¹ìŒ) ë§ˆì´í¬ ë²„íŠ¼ ë‘ë²ˆ ëˆ„ë¥´ë©´ ì‚¬ìš©ì ìŒì„±ì‹ í˜¸ mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥ - ./output.mp3
            audio_bytes = audio_recorder(text="")

            if audio_bytes:
                st.audio(audio_bytes, format="audio/wav")

            ## ë…¹ìŒ ì™„ë£Œë˜ë©´ mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  SST í•¨ìˆ˜ ì´ìš©í•˜ì—¬ text ë³€í™˜
            if audio_bytes is not None:
                ## mp3 í˜•ì‹ìœ¼ë¡œ ì €ì¥
                audio_segmant = AudioSegment.from_file(io.BytesIO(audio_bytes))
                # Export the audio file
                audio_segmant.export('./data/audio/output.mp3', format='mp3')

                # mp3 íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ STT ì ìš©
                client = OpenAI()
                sst_text = utility.STT("./data/audio/output.mp3", client)

                clean_text = sst_text.replace("\n", "")

                # ìœ ì € inputì°½ì— í…ìŠ¤íŠ¸ ì‹¬ì–´ì¤Œ.
                js = f"""
                <script>
                function insertText(dummy_var_to_force_repeat_execution) {{
                var chatInput = parent.document.querySelector('textarea[data-testid="stChatInput"]');
                var nativeInputValueSetter = Object.getOwnPropertyDescriptor(window.HTMLTextAreaElement.prototype, "value").set;
                nativeInputValueSetter.call(chatInput, "{clean_text}");
                var event = new Event('input', {{ bubbles: true}});
                chatInput.dispatchEvent(event);
                }}
                insertText({len(st.session_state['generated'])});
                </script>
                """
                st.components.v1.html(js)

                audio_bytes = None 
        with col2:
            st.sidebar.button("ğŸ§", on_click=tts) # ğŸ”ˆ

    if 'generated' not in st.session_state:
        st.session_state['generated'] = []

    if 'past' not in st.session_state:
        st.session_state['past'] = []

    if prompt := st.chat_input("ë³µì§€ì œë„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì¥"):
        response, docs = modeloutput(prompt)
        final_response = response + " " + docs

        st.session_state.past.append(prompt)
        st.session_state.generated.append(final_response)

    for i in range(len(st.session_state['past'])):
        message(st.session_state['past'][i], is_user=True, key=str(i) + '_user', logo=user_img)
        if len(st.session_state['generated']) > i:
            message(st.session_state['generated'][i], key=str(i) + '_bot', logo=bot_img)

    # ==============================================================================
    # ## Initialize Chatting Session Record (similar to history, but different!)
    # if 'messages' not in st.session_state:
    #     st.session_state.messages = []
    
    # ## Display chat msg from history on app rerun
    # for msg in st.session_state.messages:
    #     with st.chat_message(msg['role']):
    #         st.markdown(msg['content'])

    # ## React to user input
    # if prompt := st.chat_input("ë³µì§€ì œë„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì¥"):

    #     ## Display User msg in chat msg container
    #     with st.chat_message('user'):
    #         st.write(prompt)
        
    #     ## Add user msg to chat history
    #     st.session_state.messages.append({'role': 'user', 'content': prompt})
    #     response, docs = modeloutput(prompt)
    #     final_response = f"""
    #     {response}  
    #     {docs} 
    #     """
    #     ## Display Assistant msg in chat msg container
    #     with st.chat_message('assistant'):
    #         st.markdown(final_response)
        
    #     ## Add assistant response to chat history
    #     st.session_state.messages.append({'role': 'assistant', 'content': final_response})
